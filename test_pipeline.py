"""
Test Script for ‡§™‡•ç‡§∞char LangFlow Pipeline
Demonstrates the complete flow from image upload to multi-platform posting

This script runs the entire pipeline in test mode to validate functionality
"""

import os
import sys
import asyncio
import json
from pathlib import Path
from datetime import datetime

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from langflow_pipeline import PracharPipeline
from scripts.instagram_poster import InstagramGraphAPI, create_instagram_caption
from utilities.video_generator import create_demo_reel
from utilities.logger import get_logger, step

log = get_logger("pipeline_test")


class PipelineTestSuite:
    """Complete test suite for the ‡§™‡•ç‡§∞char pipeline"""
    
    def __init__(self):
        self.test_results = {
            "timestamp": datetime.now().isoformat(),
            "tests": {},
            "overall_status": "pending"
        }
        
    async def run_all_tests(self) -> Dict[str, Any]:
        """Run complete test suite"""
        log.info("üß™ Starting ‡§™‡•ç‡§∞char Pipeline Test Suite")
        
        tests = [
            ("environment_check", self.test_environment),
            ("image_enhancement", self.test_image_enhancement),
            ("video_generation", self.test_video_generation),
            ("instagram_api", self.test_instagram_api),
            ("telegram_integration", self.test_telegram_integration),
            ("full_pipeline", self.test_full_pipeline)
        ]
        
        passed_tests = 0
        
        for test_name, test_func in tests:
            log.info(f"Running test: {test_name}")
            
            try:
                with step(f"Test: {test_name}", items=1):
                    result = await test_func() if asyncio.iscoroutinefunction(test_func) else test_func()
                    
                    self.test_results["tests"][test_name] = {
                        "status": "passed" if result else "failed",
                        "details": result if isinstance(result, dict) else {"success": result}
                    }
                    
                    if result:
                        passed_tests += 1
                        log.info(f"‚úÖ {test_name} passed")
                    else:
                        log.error(f"‚ùå {test_name} failed")
                        
            except Exception as e:
                log.error(f"üí• {test_name} crashed: {e}")
                self.test_results["tests"][test_name] = {
                    "status": "error",
                    "error": str(e)
                }
        
        # Overall status
        if passed_tests == len(tests):
            self.test_results["overall_status"] = "all_passed"
        elif passed_tests > 0:
            self.test_results["overall_status"] = "partial_passed"
        else:
            self.test_results["overall_status"] = "all_failed"
            
        self.test_results["passed_count"] = passed_tests
        self.test_results["total_count"] = len(tests)
        
        return self.test_results
    
    def test_environment(self) -> bool:
        """Test environment setup and API keys"""
        log.info("Testing environment configuration...")
        
        required_vars = [
            "TELEGRAM_BOT_TOKEN",
            "TELEGRAM_CHANNEL"
        ]
        
        optional_vars = [
            "GOOGLE_API_KEY",
            "INSTAGRAM_ACCESS_TOKEN",
            "INSTAGRAM_USER_ID"
        ]
        
        missing_required = [var for var in required_vars if not os.getenv(var)]
        missing_optional = [var for var in optional_vars if not os.getenv(var)]
        
        if missing_required:
            log.error(f"Missing required environment variables: {missing_required}")
            return False
        
        if missing_optional:
            log.warning(f"Missing optional environment variables: {missing_optional}")
            log.warning("Some features will be limited in testing mode")
        
        log.info("Environment configuration valid ‚úì")
        return True
    
    def test_image_enhancement(self) -> bool:
        """Test image enhancement with Nano Banana (simulated)"""
        log.info("Testing image enhancement...")
        
        try:
            from langflow_pipeline import NanoBananaEnhancer
            
            # Test with sample images
            sample_images = [
                "campaigns/kalamkari-scarf/assets/img1.jpg",
                "campaigns/kalamkari-scarf/assets/img2.jpg"
            ]
            
            # Check if test images exist
            existing_images = [img for img in sample_images if Path(img).exists()]
            
            if not existing_images:
                log.warning("No test images found, creating mock test")
                return True  # Pass if no images to test with
            
            enhancer = NanoBananaEnhancer()
            enhancement_prompts = {"general": "Enhance for social media"}
            
            enhanced = enhancer.enhance_artisan_images(existing_images, enhancement_prompts)
            
            if len(enhanced) == len(existing_images):
                log.info(f"Enhanced {len(enhanced)} images successfully ‚úì")
                return True
            else:
                log.error("Image enhancement returned incorrect number of results")
                return False
                
        except Exception as e:
            log.error(f"Image enhancement test failed: {e}")
            return False
    
    def test_video_generation(self) -> bool:
        """Test video/reel generation"""
        log.info("Testing video generation...")
        
        try:
            from utilities.video_generator import SimpleReelGenerator, create_demo_reel
            
            generator = SimpleReelGenerator()
            
            # Test with sample data
            test_images = [
                "campaigns/kalamkari-scarf/assets/img1.jpg",
                "campaigns/kalamkari-scarf/assets/img2.jpg"
            ]
            
            test_metadata = {
                "id": "test-video",
                "titles": {
                    "en": "Test Reel Generation",
                    "hi": "‡§ü‡•á‡§∏‡•ç‡§ü ‡§∞‡•Ä‡§≤ ‡§ú‡•á‡§®‡§∞‡•á‡§∂‡§®"
                },
                "price": {"low": 500, "high": 1000}
            }
            
            # Create test output directory
            test_output = Path("test_output")
            test_output.mkdir(exist_ok=True)
            
            # Generate reel
            reel_path = create_demo_reel(test_images, test_metadata, str(test_output))
            
            if Path(reel_path).exists():
                log.info(f"Video generation successful: {reel_path} ‚úì")
                return True
            else:
                log.error("Video generation failed - no output file")
                return False
                
        except Exception as e:
            log.error(f"Video generation test failed: {e}")
            return False
    
    def test_instagram_api(self) -> bool:
        """Test Instagram API integration"""
        log.info("Testing Instagram API...")
        
        try:
            # Test basic API setup
            from scripts.instagram_poster import InstagramGraphAPI, create_instagram_caption
            
            # Test caption generation (doesn't need API keys)
            test_metadata = {
                "titles": {"en": "Test Product", "hi": "‡§ü‡•á‡§∏‡•ç‡§ü ‡§â‡§§‡•ç‡§™‡§æ‡§¶"},
                "description": {"hi": "‡§Ø‡§π ‡§è‡§ï ‡§™‡§∞‡•Ä‡§ï‡•ç‡§∑‡§£ ‡§π‡•à"},
                "hashtags": ["#test", "#prachar", "#artisan"]
            }
            
            caption = create_instagram_caption(test_metadata)
            
            if caption and len(caption) > 10:
                log.info("Caption generation working ‚úì")
            else:
                log.error("Caption generation failed")
                return False
            
            # Test API initialization (if credentials available)
            if os.getenv("INSTAGRAM_ACCESS_TOKEN"):
                try:
                    api = InstagramGraphAPI()
                    log.info("Instagram API credentials validated ‚úì")
                except Exception as e:
                    log.warning(f"Instagram API credentials issue: {e}")
                    log.info("Instagram posting will be simulated in tests")
            else:
                log.info("No Instagram credentials - will simulate posting")
            
            return True
            
        except Exception as e:
            log.error(f"Instagram API test failed: {e}")
            return False
    
    def test_telegram_integration(self) -> bool:
        """Test Telegram integration"""
        log.info("Testing Telegram integration...")
        
        try:
            from scripts.telegram_poster import post_campaign
            
            # Test with dry run (won't actually post)
            test_metadata = {
                "title_en": "Test Campaign",
                "title_hi": "‡§ü‡•á‡§∏‡•ç‡§ü ‡§ï‡•à‡§Æ‡•ç‡§™‡•á‡§®",
                "description_hi": "‡§Ø‡§π ‡§è‡§ï ‡§™‡§∞‡•Ä‡§ï‡•ç‡§∑‡§£ ‡§π‡•à",
                "price": {"low": 500, "high": 1000, "currency": "INR"},
                "hashtags": ["#test", "#prachar"],
                "cta_whatsapp": "https://wa.me/test"
            }
            
            test_media = ["campaigns/kalamkari-scarf/assets/img1.jpg"]
            
            # Check if we have Telegram credentials
            if os.getenv("TELEGRAM_BOT_TOKEN"):
                log.info("Telegram credentials available ‚úì")
                # Note: We won't actually post in test mode
                return True
            else:
                log.warning("No Telegram credentials - integration test limited")
                return True  # Still pass as this is expected in some setups
                
        except Exception as e:
            log.error(f"Telegram integration test failed: {e}")
            return False
    
    async def test_full_pipeline(self) -> bool:
        """Test the complete pipeline end-to-end"""
        log.info("Testing full pipeline...")
        
        try:
            # Set environment to test mode
            os.environ["SKIP_ACTUAL_POSTING"] = "true"
            
            pipeline = PracharPipeline()
            
            # Test with sample campaign
            campaign_dir = "campaigns/kalamkari-scarf"
            
            if not Path(campaign_dir).exists():
                log.warning("Sample campaign not found, creating mock test")
                return True
            
            # Run pipeline in test mode
            result = await pipeline.process_artisan_campaign(campaign_dir)
            
            if result.get("status") == "success":
                log.info("Full pipeline test successful ‚úì")
                log.info(f"Pipeline result: {json.dumps(result, indent=2)}")
                return True
            else:
                log.error(f"Pipeline test failed: {result}")
                return False
                
        except Exception as e:
            log.error(f"Full pipeline test failed: {e}")
            return False
    
    def print_test_report(self):
        """Print detailed test report"""
        print("\n" + "="*60)
        print("‡§™‡•ç‡§∞char PIPELINE TEST REPORT")
        print("="*60)
        
        print(f"Test Time: {self.test_results['timestamp']}")
        print(f"Overall Status: {self.test_results['overall_status'].upper()}")
        print(f"Tests Passed: {self.test_results['passed_count']}/{self.test_results['total_count']}")
        print()
        
        for test_name, test_result in self.test_results["tests"].items():
            status = test_result["status"]
            icon = "‚úÖ" if status == "passed" else "‚ùå" if status == "failed" else "üí•"
            
            print(f"{icon} {test_name.replace('_', ' ').title()}: {status.upper()}")
            
            if status != "passed" and "error" in test_result:
                print(f"   Error: {test_result['error']}")
        
        print("\n" + "="*60)
        
        if self.test_results["overall_status"] == "all_passed":
            print("üéâ ALL TESTS PASSED! Pipeline is ready for deployment.")
        elif self.test_results["overall_status"] == "partial_passed":
            print("‚ö†Ô∏è  Some tests passed. Check failed tests above.")
        else:
            print("üö® ALL TESTS FAILED. Check configuration and dependencies.")
        
        print("="*60)


async def main():
    """Run the complete test suite"""
    print("üöÄ Starting ‡§™‡•ç‡§∞char LangFlow Pipeline Test Suite")
    print("This will test all components of the automated reel creation system")
    print()
    
    test_suite = PipelineTestSuite()
    
    try:
        results = await test_suite.run_all_tests()
        test_suite.print_test_report()
        
        # Save results to file
        results_file = f"test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\nDetailed results saved to: {results_file}")
        
        # Exit code based on results
        if results["overall_status"] == "all_passed":
            return 0
        elif results["overall_status"] == "partial_passed":
            return 1
        else:
            return 2
            
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è Test interrupted by user")
        return 130
    except Exception as e:
        print(f"\n\nüí• Test suite crashed: {e}")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)